{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-02\n",
      "2015-01-03\n",
      "2015-01-04\n",
      "2015-01-05\n",
      "2015-01-06\n",
      "2015-01-07\n",
      "2015-01-08\n",
      "2015-01-09\n",
      "2015-01-10\n",
      "2015-01-11\n",
      "2015-01-12\n",
      "2015-01-13\n",
      "2015-01-14\n",
      "2015-01-15\n",
      "2015-01-16\n",
      "2015-01-17\n",
      "2015-01-18\n",
      "2015-01-19\n",
      "2015-01-20\n",
      "2015-01-21\n",
      "2015-01-22\n",
      "2015-01-23\n",
      "2015-01-24\n",
      "2015-01-25\n",
      "2015-01-26\n",
      "2015-01-27\n",
      "2015-01-28\n",
      "2015-01-29\n",
      "2015-01-30\n"
     ]
    }
   ],
   "source": [
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + datetime.timedelta(n)\n",
    "\n",
    "start_date = datetime.date(2015, 1, 1)\n",
    "end_date = datetime.date(2015, 1, 30)\n",
    "for single_date in daterange(start_date, end_date):\n",
    "    print(single_date+datetime.timedelta(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_urls(startDate,endDate,count=200,sources=[\"CNN\",\"NYT\",\"BBC\",\"Reuters\",\"Fox News\",\"Telegraph\",\"Guardian\",\"The Times\"]):\n",
    "    \n",
    "    def daterange(start_date, end_date):\n",
    "        for n in range(int ((end_date - start_date).days)):\n",
    "            yield start_date + datetime.timedelta(n)\n",
    "\n",
    "    er = EventRegistry()\n",
    "    q = QueryArticles()\n",
    "    # set news sources\n",
    "    for x in sources:\n",
    "        sourceUri = er.getNewsSourceUri(x)\n",
    "        q.addNewsSource(sourceUri)\n",
    "\n",
    "    # set result settings\n",
    "    q.addRequestedResult(RequestArticlesInfo(page = 1, count = count,\n",
    "        returnInfo = ReturnInfo(articleInfo = ArticleInfoFlags(concepts = False, categories = False, image = False))))\n",
    "        \n",
    "    urls = []\n",
    "        \n",
    "    for single_date in daterange(startDate, endDate):\n",
    "        print(single_date,single_date+datetime.timedelta(1))\n",
    "        # set the date limit of interest\n",
    "        q.setDateLimit(single_date, single_date+datetime.timedelta(1))\n",
    "        \n",
    "        res = er.execQuery(q)\n",
    "        \n",
    "        try:\n",
    "            for x in range(0,count):\n",
    "                urls.append(res['articles']['results'][x]['url'])\n",
    "        except:\n",
    "            print(res['error'])\n",
    "        \n",
    "    unique_urls = list(set(urls))\n",
    "    \n",
    "    return unique_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found username and password in settings file which will be used for making requests. Trying to login...\n",
      "Event Registry host: http://eventregistry.org\n",
      "2017-01-01 2017-01-02\n",
      "No results match the given time constraints\n",
      "2017-01-02 2017-01-03\n",
      "No results match the given time constraints\n",
      "2017-01-03 2017-01-04\n",
      "No results match the given time constraints\n",
      "2017-01-04 2017-01-05\n",
      "2017-01-05 2017-01-06\n",
      "2017-01-06 2017-01-07\n",
      "2017-01-07 2017-01-08\n",
      "2017-01-08 2017-01-09\n",
      "2017-01-09 2017-01-10\n",
      "2017-01-10 2017-01-11\n",
      "2017-01-11 2017-01-12\n",
      "2017-01-12 2017-01-13\n",
      "2017-01-13 2017-01-14\n",
      "2017-01-14 2017-01-15\n",
      "2017-01-15 2017-01-16\n",
      "2017-01-16 2017-01-17\n",
      "2017-01-17 2017-01-18\n",
      "2017-01-18 2017-01-19\n",
      "2017-01-19 2017-01-20\n",
      "2017-01-20 2017-01-21\n",
      "2017-01-21 2017-01-22\n",
      "2017-01-22 2017-01-23\n",
      "2017-01-23 2017-01-24\n",
      "2017-01-24 2017-01-25\n",
      "2017-01-25 2017-01-26\n",
      "2017-01-26 2017-01-27\n",
      "2017-01-27 2017-01-28\n",
      "2017-01-28 2017-01-29\n",
      "2017-01-29 2017-01-30\n",
      "2017-01-30 2017-01-31\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime.date(2017, 1, 1)\n",
    "end_date = datetime.date(2017, 1, 31)\n",
    "urls = get_urls(start_date,end_date,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4992"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.reuters.com/video/2014/03/21/mixed-reaction-to-crimeas-annexation?videoId=297219996&videoChannel=2602',\n",
       " 'http://www.foxnews.com/us/2017/01/07/airport-shooting-raises-questions-about-guns-in-baggage.html',\n",
       " 'http://www.foxnews.com/us/2017/01/26/man-admits-to-luring-gay-man-into-woods-fatally-beating-him.html',\n",
       " 'http://www.reuters.com/article/us-jpmorgan-dimon-compensation-idUSKBN15336N',\n",
       " 'http://www.cnn.com/2017/01/05/weather/winter-storm-south-snow/index.html',\n",
       " 'http://www.telegraph.co.uk/business/2017/01/15/mauritius-payment-knocks-fundsmith-profits-despite-surge-new/',\n",
       " 'http://www.bbc.co.uk/news/uk-northern-ireland-38726562',\n",
       " 'http://www.reuters.com/article/us-usa-congress-price-hearing-idUSKBN1582MP',\n",
       " 'http://www.telegraph.co.uk/business/2017/01/28/collusion-suspected-bt-scandal/',\n",
       " 'http://www.cnn.com/2017/01/29/politics/trump-travel-ban-congress-reaction/index.html']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('er_urls_012017.pkl', 'wb') as handle:\n",
    "    pickle.dump(urls, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SVM_evaluate(self):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import svm\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    json_dat = [doc for doc in self._db.clean_data.find()]\n",
    "    df_dat = pd.DataFrame(json_dat)\n",
    "    Y = np.array(df_dat['True'])\n",
    "\n",
    "    df_dat.drop(['_id','body_length','resource','title_compounds','url','True'], axis=1, inplace=True)\n",
    "\n",
    "    X = np.array(df_dat)\n",
    "\n",
    "\n",
    "    K = 10\n",
    "    skf = StratifiedKFold(n_splits=K, random_state=None, shuffle=True)\n",
    "\n",
    "    acc = []\n",
    "    for train_index, test_index in skf.split(X, Y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "        # Train classifier\n",
    "        clf = svm.SVC(probability=True)\n",
    "        clf.fit(X_train, Y_train)  \n",
    "\n",
    "        Y_pred = clf.predict(X_test)\n",
    "\n",
    "        Cfold = confusion_matrix(Y_test,Y_pred)\n",
    "        self.classifier_performance['confusion'] += Cfold\n",
    "        acc.append(sum(np.diag(Cfold))/sum(sum(Cfold)))\n",
    "\n",
    "    acc = np.array(acc)\n",
    "    self.classifier_performance['accuracy_mean'] = np.mean(acc)\n",
    "    self.classifier_performance['accuracy_std'] = np.std(acc)\n",
    "    \n",
    "def SVM_fit(self):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import svm\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    json_dat = [doc for doc in self._db.clean_data.find()]\n",
    "    df_dat = pd.DataFrame(json_dat)\n",
    "    Y = np.array(df_dat['True'])\n",
    "\n",
    "    df_dat.drop(['_id','body_length','resource','title_compounds','url','True'], axis=1, inplace=True)\n",
    "\n",
    "    X = np.array(df_dat)\n",
    "\n",
    "    # Train classifier\n",
    "    self.clf = svm.SVC(probability=True)\n",
    "    self.clf.fit(X, Y)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [python3]",
   "language": "python",
   "name": "Python [python3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
